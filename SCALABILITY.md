# Backend Scalability Architecture

This document explains how the Bus Tracker backend is designed to handle high-frequency GPS updates and concurrent user requests efficiently. The system employs **End-to-End Scalability** principles, ensuring that increased load does not directly translate to increased database pressure or latency.

## ðŸš€ Key Scalability Pillars

The architecture relies on three main strategies:
1. **In-Memory State Management** (Write-Back Caching)
2. **Read-Path Optimization** (Zero-DB Hot Paths)
3. **Algorithmic Efficiency** (Spatial Indexing Hints)

---

## 1. High-Frequency Ingestion (Write Path)

**Challenge:** Thousands of buses send GPS updates every 1-3 seconds. Writing every update to MongoDB would crash the database due to IOPS limits.

**Solution: buffering & Batching**
- **In-Memory First:** When `POST /api/gps/update` is called, the data is immediately updated in a `Map` within `BusStateManager.js`. The API responds with `200 OK` in microseconds, without waiting for the database.
- **Write-Back Policy:** A background process (`setInterval`) runs every **5 seconds**. It collects all "dirty" (changed) bus states and performs a single **Bulk Write** to MongoDB.
- **Collapsing Updates:** If a bus sends 5 updates in 5 seconds, the database only sees **1 write** (the latest state). This reduces write load by ~80-90%.

**Code Reference:** `server/services/BusStateManager.js`
> "The flush loop checks `isDirty` set and executes `BusState.bulkWrite(ops)`."

---

## 2. Low-Latency Data Retrieval (Read Path)

**Challenge:** Users constantly poll for live bus locations (`GET /api/bus/:id/live`). Fetching from DB for every poll is slow and unscalable.

**Solution: Database-Free Hot Path**
- **State Caching:** The `BusStateManager` serves the latest bus location directly from RAM.
- **Route Caching:** Static route data (polylines, stops) is loaded into `RouteCache` on startup. The server never queries the `routes` collection during a live tracking request.
- **Result:** The "Live Tracking" API is purely CPU-bound (RAM access), not I/O-bound, allowing a single Node.js instance to handle thousands of concurrent reads.

**Code Reference:** `server/services/RouteCache.js`
> "Routes constitute static data and are served entirely from `this.routes` Map."

---

## 3. Algorithmic Optimization (CPU Scaling)

**Challenge:** Mapping a raw GPS point to a route's polyline (Snap-to-Road) involves calculating distance to thousands of points (O(N) complexity). Doing this for every update burns CPU.

**Solution: Spatial "Hints"**
- The system remembers the **last known index** of the bus on the polyline.
- For the next update, it searches only a small **Window** (e.g., current index + 50 points) instead of the entire route.
- **Result:** Complexity drops from **O(N)** to **O(k)** (constant time), maintaining low CPU usage even with long routes.

**Code Reference:** `server/services/services.js` -> `projectAlongPolyline`
> "Uses `hintIndex` to limit the search loop to a local segment."

---

## Architecture Diagram (End-to-End)

```mermaid
graph TD
    GPS[GPS Simulators] -->|POST /update (High Freq)| API[Node.js Server]
    API -->|Update Map| RAM[(In-Memory State)]
    
    subgraph "Write Path (Async)"
        RAM -->|Flush (5s)| Batcher[Batch Process]
        Batcher -->|Bulk Write| DB[(MongoDB)]
    end
    
    subgraph "Read Path (Fast)"
        User[Client App] -->|GET /live| API
        API -->|Read| RAM
        API -->|Read Route| Cache[RouteCache]
    end
```

## Summary

| Feature | Without Optimization | With Current Architecture |
|:---|:---|:---|
| **DB Writes** | 1 per GPS update (1000/sec = Crash) | 1 per 5s per Bus (Predictable Load) |
| **Read Latency** | 10-50ms (DB Query) | < 1ms (Memory Access) |
| **Snap-to-Road** | O(N) Heavy Calculation | O(1) Constant Time |

This design allows the backend to scale to support **thousands of concurrent buses** on a single modest server instance.
